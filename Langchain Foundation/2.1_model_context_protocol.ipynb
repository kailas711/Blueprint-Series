{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d135119e",
   "metadata": {},
   "source": [
    "### MCP in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2022461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n",
    "\n",
    "##### Restart kernel after this ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb73399",
   "metadata": {},
   "source": [
    "### Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d30d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb45cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='search_web', description='Search the web for information', args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'search_webArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x0000027B858DD1B0>)]\n",
      "[Blob 2729544624672]\n",
      "\n",
      "    You are a helpful assistant that answers user questions about LangChain, LangGraph and LangSmith.\n",
      "\n",
      "    You can use the following tools/resources to answer user questions:\n",
      "    - search_web: Search the web for information\n",
      "    - github_file: Access the langchain-ai repo files\n",
      "\n",
      "    If the user asks a question that is not related to LangChain, LangGraph or LangSmith, you should say \"I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"\n",
      "\n",
      "    You may try multiple tool and resource calls to answer the user's question.\n",
      "\n",
      "    You may also ask clarifying questions to the user to better understand their question.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# get tools \n",
    "tools = await client.get_tools()\n",
    "print(tools)\n",
    "\n",
    "# get resources \n",
    "resources = await client.get_resources(\"local_server\")\n",
    "print(resources)\n",
    "\n",
    "# get prompts \n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278111c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:1b\", \n",
    "                   temperature=0,\n",
    "                   validate_model_on_init=True,\n",
    "                   seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6342ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b616b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='eb37b26f-995d-4ffc-b118-d6f59d949cf5'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-23T06:19:36.7785192Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6599087500, 'load_duration': 5167414100, 'prompt_eval_count': 298, 'prompt_eval_duration': 1038457300, 'eval_count': 26, 'eval_duration': 372250200, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b49dc-ddf8-7cd3-970a-e025d175238a-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'd5f55065-fca0-4068-8cd3-197f97f4c9ce', 'type': 'tool_call'}], usage_metadata={'input_tokens': 298, 'output_tokens': 26, 'total_tokens': 324}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-tools/\",\\n      \"title\": \"langchain-mcp-tools\",\\n      \"content\": \"LangChain\\'s official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using ...Read more\",\\n      \"score\": 0.99997663,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to ...\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\\\\\"http://localhost:3000/mcp\\\\\", transport_type=\\\\\"sse\\\\\", headers={ \\\\\"Authorization\\\\\": f\\\\\"Bearer {os.getenv(\\'API_TOKEN\\')}\\\\\", \\\\\"User-Agent\\\\\": \\\\\"LangChain-MCP-Client/1.0\\\\\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Create a new lead for John Smith with email [email\\xa0protected]\\\\\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.\",\\n      \"score\": 0.9999621,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.9999461,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.\",\\n      \"score\": 0.99993896,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"358582af-bc5e-40fd-ac9a-561f5a60f238\"\\n}', 'id': 'lc_81f6eade-f09b-44b8-8431-8ec5d4d68df9'}], name='search_web', id='69f97de1-f0ad-4fac-9ffc-b2e1d63a1e76', tool_call_id='d5f55065-fca0-4068-8cd3-197f97f4c9ce', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999956, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-tools/', 'title': 'langchain-mcp-tools', 'content': \"LangChain's official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using ...Read more\", 'score': 0.99997663, 'raw_content': None}, {'url': 'https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters', 'title': 'LangChain MCP Integration: Complete Guide to ...', 'content': 'from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\"python\", \"database_mcp_server.py\"], transport_type=\"stdio\", environment={ \"DATABASE_URL\": \"postgresql://user:pass@localhost:5432/mydb\", \"MAX_CONNECTIONS\": \"10\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\"gpt-4\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \"input\": \"Find all customers who made purchases over $500 in the last month\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\"http://localhost:3000/mcp\", transport_type=\"sse\", headers={ \"Authorization\": f\"Bearer {os.getenv(\\'API_TOKEN\\')}\", \"User-Agent\": \"LangChain-MCP-Client/1.0\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \"input\": \"Create a new lead for John Smith with email [email\\xa0protected]\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.', 'score': 0.9999621, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.9999461, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': '[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.', 'score': 0.99993896, 'raw_content': None}], 'response_time': 0.0, 'request_id': '358582af-bc5e-40fd-ac9a-561f5a60f238'}}}),\n",
      "              AIMessage(content='The `langchain-mcp-adapters` library is a package that makes it easy to use Anthropic Model Context Protocol (MCP) tools with LangChain and LangGraph. It converts MCP tools into LangChain- & LangGraph-compatible tools, enables interaction with tools across multiple MCP servers, and seamlessly integrates the hundreds of tool servers already published into LangGraph Agents.\\n\\nThis library provides a simple way to integrate MCP tools with LangChain and LangGraph, allowing users to leverage the capabilities of external services while maintaining control over their own applications. The `langchain-mcp-adapters` library is designed to be easy to use and requires minimal configuration, making it an ideal choice for developers who want to take full advantage of the growing ecosystem of MCP tool servers.\\n\\nIn summary, the `langchain-mcp-adapters` library is a convenient way to integrate MCP tools with LangChain and LangGraph, enabling users to access a wide range of external services while maintaining control over their own applications.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-23T06:19:48.5330075Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7561907100, 'load_duration': 107934900, 'prompt_eval_count': 1995, 'prompt_eval_duration': 3645993100, 'eval_count': 199, 'eval_duration': 3794325100, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b49dd-0829-7001-aeb8-48828d6265cd-0', usage_metadata={'input_tokens': 1995, 'output_tokens': 199, 'total_tokens': 2194})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20174a57",
   "metadata": {},
   "source": [
    "### Online MCP servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb83d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n",
    "\n",
    "##### Restart kernel after this ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"path to uvx.exe\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ec69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "# Load the model\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:1b\", \n",
    "                   temperature=0,\n",
    "                   validate_model_on_init=True,\n",
    "                   seed=42)\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276bcdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Pregel.ainvoke at 0x00000150F6700270>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
